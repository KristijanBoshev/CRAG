{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdIaAUdxEZdjhXLw1TNS/Z"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Light Corrective RAG Using LangChain and CrewAI**"
      ],
      "metadata": {
        "id": "7WUgXIiBMYfM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Ob5XbxLHLqww"
      },
      "outputs": [],
      "source": [
        "!pip install crewai crewai_tools langchain_community sentence-transformers langchain-groq langchain_huggingface --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "from crewai_tools import PDFSearchTool\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from crewai_tools  import tool\n",
        "from crewai import Crew\n",
        "from crewai import Task\n",
        "from crewai import Agent"
      ],
      "metadata": {
        "id": "us_t9aLAMRIr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Groq API Key**"
      ],
      "metadata": {
        "id": "3bdStU4Ra9XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Use different key since this one will be deactivated\n",
        "os.environ['GROQ_API_KEY'] = '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'"
      ],
      "metadata": {
        "id": "zLs_IcGlMWx1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLM being used**"
      ],
      "metadata": {
        "id": "JxRe3CkjbF5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    openai_api_key=os.environ['GROQ_API_KEY'],\n",
        "    model_name=\"llama3-8b-8192\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=1000,\n",
        ")"
      ],
      "metadata": {
        "id": "8toqz_WqNpYs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "pdf_url = 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'\n",
        "response = requests.get(pdf_url)\n",
        "\n",
        "with open('attention_is_all_you_need.pdf', 'wb') as file:\n",
        "    file.write(response.content)"
      ],
      "metadata": {
        "id": "VBwiVI0nN5bs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG tool to pass the PDF document**"
      ],
      "metadata": {
        "id": "TGD_Xw4YbMJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_tool = PDFSearchTool(pdf='/content/attention_is_all_you_need.pdf',\n",
        "    config=dict(\n",
        "        llm=dict(\n",
        "            provider=\"groq\",\n",
        "            config=dict(\n",
        "                model=\"llama3-8b-8192\",\n",
        "                temperature=0.5,\n",
        "                top_p=1,\n",
        "                stream=True,\n",
        "            ),\n",
        "        ),\n",
        "        embedder=dict(\n",
        "            provider=\"huggingface\",\n",
        "            config=dict(\n",
        "                model=\"BAAI/bge-small-en-v1.5\",\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "LR__I60_OBLz"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_tool.run(\"What is multi-head attention?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "gCluB9YIRQuQ",
        "outputId": "cf3c9c2a-f15a-49ea-af70-d1db7b2fdcc9"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Tool: Search a PDF's content\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Relevant Content:\\nas depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. 4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1. Then their dot product, q·k=∑dk i=1qiki, has mean 0and variance dk. 4\\n\\nMultiHead( Q,K,V ) = Concat(head 1,.,head h)WO where head i= Attention( QWQ i,KWK i,VWV i) Where the projections are parameter matrices WQ i∈Rdmodel×dk,WK i∈Rdmodel×dk,WV i∈Rdmodel×dv andWO∈Rhdv×dmodel. In this work we employ h= 8 parallel attention layers, or heads. For each of these we use dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: •In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8]. •The encoder contains self-attention layers. In a self-attention\\n\\nScaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention(Q,K,V ) = softmax(QKT √dk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of1√dk. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efﬁcient in'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Web Search API key**"
      ],
      "metadata": {
        "id": "Z4F13R-tbswp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Use different key since this one will be deactivated\n",
        "os.environ['TAVILY_API_KEY'] = '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'"
      ],
      "metadata": {
        "id": "ATPGXuUUTsHk"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "web_search_tool = TavilySearchResults(k=3)"
      ],
      "metadata": {
        "id": "38Vj4HOhTsLt"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "web_search_tool.run(\"What is multi-head attention?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQxLLkw4TsSY",
        "outputId": "fdb60aa1-d465-4118-d3ca-69ef0edc74d4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://medium.com/@digvijay.qi/multi-head-attention-4eb2ec3d3a3f',\n",
              "  'content': 'Multi-head attention is a powerful… | by Digvijay Y | Medium Multi-head attention is a powerful extension of the self-attention mechanism that significantly improves its ability to capture diverse aspects of relationships within a sequence. Multi-head attention addresses this by using multiple independent attention heads, each with its own set of learned weight matrices for Query, Key, and Value vectors. Independent Calculations: Each head performs the core self-attention steps (queries, keys, values, attention scores, softmax, context vectors) but with its own learned weight matrices. Multi-head attention works in the following steps:'},\n",
              " {'url': 'https://medium.com/@hassaanidrees7/exploring-multi-head-attention-why-more-heads-are-better-than-one-006a5823372b',\n",
              "  'content': 'Understanding the Power and Benefits of Multi-Head Attention in Transformer Models For students and practitioners in AI and machine learning, grasping the concept and benefits of multi-head attention is essential for leveraging the full potential of Transformer models. Multi-head attention is an extension of the self-attention mechanism used in Transformer models. Multi-head attention is a key component in various Transformer-based models, enabling them to excel in diverse tasks: Multi-head attention is a powerful mechanism that significantly enhances the performance of Transformer models by allowing them to capture diverse features, distribute attention effectively, and improve generalization.'},\n",
              " {'url': 'https://towardsdatascience.com/multi-head-attention-formally-explained-and-defined-89dc70ce84bd',\n",
              "  'content': 'Multi-head attention eventually simply consists of concatenating the outputs of different predefined attention heads, into a large matrix. Then, it is transformed into a n x d output matrix that matches dimensions with the input. Solution — Exercise 6. The dimensions of these matrices, which intervene for multi-head attention, are:'},\n",
              " {'url': 'https://storrs.io/attention/',\n",
              "  'content': 'Multi-head attention allows for the neural network to control the mixing of information between pieces of an input sequence, leading to the creation of richer representations, which in turn allows for increased performance on machine learning tasks.'},\n",
              " {'url': 'https://medium.com/@weidagang/demystifying-transformers-multi-head-attention-43b3173de391',\n",
              "  'content': 'Like having a team of specialists instead of a single worker, multi-head attention utilizes multiple attention heads in parallel. Each head learns to focus on different aspects of the input:'}]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Router tool to decide vector store or web search**"
      ],
      "metadata": {
        "id": "lPxfDERKbznJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def router_tool(question):\n",
        "  \"\"\"Router Function\"\"\"\n",
        "  if 'self-attention' in question:\n",
        "    return 'vectorstore'\n",
        "  else:\n",
        "    return 'web_search'"
      ],
      "metadata": {
        "id": "wJZYTWDyRbCZ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CrewAI Agents**"
      ],
      "metadata": {
        "id": "Ll_5HDPAcHp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Router_Agent = Agent(\n",
        "  role='Router',\n",
        "  goal='Route user question to a vectorstore or web search',\n",
        "  backstory=(\n",
        "    \"You are an expert at routing a user question to a vectorstore or web search.\"\n",
        "    \"Use the vectorstore for questions on concept related to Retrieval-Augmented Generation.\"\n",
        "    \"You do not need to be stringent with the keywords in the question related to these topics. Otherwise, use web-search.\"\n",
        "  ),\n",
        "  verbose=True,\n",
        "  allow_delegation=False,\n",
        "  llm=llm,\n",
        ")"
      ],
      "metadata": {
        "id": "MT3tiWbdSY6P"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Retriever_Agent = Agent(\n",
        "role=\"Retriever\",\n",
        "goal=\"Use the information retrieved from the vectorstore to answer the question\",\n",
        "backstory=(\n",
        "    \"You are an assistant for question-answering tasks.\"\n",
        "    \"Use the information present in the retrieved context to answer the question.\"\n",
        "    \"You have to provide a clear concise answer.\"\n",
        "),\n",
        "verbose=True,\n",
        "allow_delegation=False,\n",
        "llm=llm,\n",
        ")"
      ],
      "metadata": {
        "id": "ruqcl5LBSY9W"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Grader_agent =  Agent(\n",
        "  role='Answer Grader',\n",
        "  goal='Filter out erroneous retrievals',\n",
        "  backstory=(\n",
        "    \"You are a grader assessing relevance of a retrieved document to a user question.\"\n",
        "    \"If the document contains keywords related to the user question, grade it as relevant.\"\n",
        "    \"It does not need to be a stringent test.You have to make sure that the answer is relevant to the question.\"\n",
        "  ),\n",
        "  verbose=True,\n",
        "  allow_delegation=False,\n",
        "  llm=llm,\n",
        ")"
      ],
      "metadata": {
        "id": "khU0Xf0xSY_o"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hallucination_grader = Agent(\n",
        "    role=\"Hallucination Grader\",\n",
        "    goal=\"Filter out hallucination\",\n",
        "    backstory=(\n",
        "        \"You are a hallucination grader assessing whether an answer is grounded in / supported by a set of facts.\"\n",
        "        \"Make sure you meticulously review the answer and check if the response provided is in alignmnet with the question asked\"\n",
        "    ),\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    llm=llm,\n",
        ")"
      ],
      "metadata": {
        "id": "U6xqjuYGSZC3"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_grader = Agent(\n",
        "    role=\"Answer Grader\",\n",
        "    goal=\"Filter out hallucination from the answer.\",\n",
        "    backstory=(\n",
        "        \"You are a grader assessing whether an answer is useful to resolve a question.\"\n",
        "        \"Make sure you meticulously review the answer and check if it makes sense for the question asked\"\n",
        "        \"If the answer is relevant generate a clear and concise response.\"\n",
        "        \"If the answer gnerated is not relevant then perform a websearch using 'web_search_tool'\"\n",
        "    ),\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    llm=llm,\n",
        ")"
      ],
      "metadata": {
        "id": "NH3rApX_SiFs"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "router_task = Task(\n",
        "    description=(\"Analyse the keywords in the question {question}\"\n",
        "    \"Based on the keywords decide whether it is eligible for a vectorstore search or a web search.\"\n",
        "    \"Return a single word 'vectorstore' if it is eligible for vectorstore search.\"\n",
        "    \"Return a single word 'websearch' if it is eligible for web search.\"\n",
        "    \"Do not provide any other premable or explaination.\"\n",
        "    ),\n",
        "    expected_output=(\"Give a binary choice 'websearch' or 'vectorstore' based on the question\"\n",
        "    \"Do not provide any other premable or explaination.\"),\n",
        "    agent=Router_Agent,\n",
        "    tools=[router_tool],\n",
        ")"
      ],
      "metadata": {
        "id": "SYyj6X_3SjA3"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_task = Task(\n",
        "    description=(\"Based on the response from the router task extract information for the question {question} with the help of the respective tool.\"\n",
        "    \"Use the web_serach_tool to retrieve information from the web in case the router task output is 'websearch'.\"\n",
        "    \"Use the rag_tool to retrieve information from the vectorstore in case the router task output is 'vectorstore'.\"\n",
        "    ),\n",
        "    expected_output=(\"You should analyse the output of the 'router_task'\"\n",
        "    \"If the response is 'websearch' then use the web_search_tool to retrieve information from the web.\"\n",
        "    \"If the response is 'vectorstore' then use the rag_tool to retrieve information from the vectorstore.\"\n",
        "    \"Return a claer and consise text as response.\"),\n",
        "    agent=Retriever_Agent,\n",
        "    context=[router_task],\n",
        ")"
      ],
      "metadata": {
        "id": "fWQyTEA0SjC2"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grader_task = Task(\n",
        "    description=(\"Based on the response from the retriever task for the quetion {question} evaluate whether the retrieved content is relevant to the question.\"\n",
        "    ),\n",
        "    expected_output=(\"Binary score 'yes' or 'no' score to indicate whether the document is relevant to the question\"\n",
        "    \"You must answer 'yes' if the response from the 'retriever_task' is in alignment with the question asked.\"\n",
        "    \"You must answer 'no' if the response from the 'retriever_task' is not in alignment with the question asked.\"\n",
        "    \"Do not provide any preamble or explanations except for 'yes' or 'no'.\"),\n",
        "    agent=Grader_agent,\n",
        "    context=[retriever_task],\n",
        ")"
      ],
      "metadata": {
        "id": "vvocDdkcSjEz"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hallucination_task = Task(\n",
        "    description=(\"Based on the response from the grader task for the quetion {question} evaluate whether the answer is grounded in / supported by a set of facts.\"),\n",
        "    expected_output=(\"Binary score 'yes' or 'no' score to indicate whether the answer is sync with the question asked\"\n",
        "    \"Respond 'yes' if the answer is in useful and contains fact about the question asked.\"\n",
        "    \"Respond 'no' if the answer is not useful and does not contains fact about the question asked.\"\n",
        "    \"Do not provide any preamble or explanations except for 'yes' or 'no'.\"),\n",
        "    agent=hallucination_grader,\n",
        "    context=[grader_task],\n",
        ")"
      ],
      "metadata": {
        "id": "PmOBAhzUSjHB"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_task = Task(\n",
        "    description=(\"Based on the response from the hallucination task for the quetion {question} evaluate whether the answer is useful to resolve the question.\"\n",
        "    \"If the answer is 'yes' return a clear and concise answer.\"\n",
        "    \"If the answer is 'no' then perform a 'websearch' and return the response\"),\n",
        "    expected_output=(\"Return a clear and concise response if the response from 'hallucination_task' is 'yes'.\"\n",
        "    \"Perform a web search using 'web_search_tool' and return ta clear and concise response only if the response from 'hallucination_task' is 'no'.\"\n",
        "    \"Otherwise respond as 'Sorry! unable to find a valid response'.\"),\n",
        "    context=[hallucination_task],\n",
        "    agent=answer_grader,\n",
        ")"
      ],
      "metadata": {
        "id": "AZXZU35cSjI0"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_crew = Crew(\n",
        "    agents=[Router_Agent, Retriever_Agent, Grader_agent, hallucination_grader, answer_grader],\n",
        "    tasks=[router_task, retriever_task, grader_task, hallucination_task, answer_task],\n",
        "    verbose=True,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvidnGxMSt9Y",
        "outputId": "3574a66b-fe96-4259-ea2c-1697f4c5a55d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs ={\"question\":\"How does multi-head attention influence large language models?\"}\n"
      ],
      "metadata": {
        "id": "i_RmB2KxSuDB"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = rag_crew.kickoff(inputs=inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO7SpzwASuFN",
        "outputId": "4526c0a9-60ce-4f84-98e2-2d39c3bdce34"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Router\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Analyse the keywords in the question How does multi-head attention influence large language models?Based on the keywords decide whether it is eligible for a vectorstore search or a web search.Return a single word 'vectorstore' if it is eligible for vectorstore search.Return a single word 'websearch' if it is eligible for web search.Do not provide any other premable or explaination.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The question seems to be related to the concept of multi-head attention in large language models, which is a topic in the field of Retrieval-Augmented Generation.\n",
            "Action: router_tool\n",
            "Action Input: {\"question\": \"How does multi-head attention influence large language models?\"}\u001b[0m\u001b[95m \n",
            "\n",
            "web_search\n",
            "\u001b[00m\n",
            "\u001b[32;1m\u001b[1;3mFinal Answer: websearch\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Router] Task output: websearch\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Retriever\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the router task extract information for the question How does multi-head attention influence large language models? with the help of the respective tool.Use the web_serach_tool to retrieve information from the web in case the router task output is 'websearch'.Use the rag_tool to retrieve information from the vectorstore in case the router task output is 'vectorstore'.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
            "\n",
            "Action: I will use the web_search_tool to retrieve information from the web.\n",
            "\n",
            "Final Answer: \n",
            "\n",
            "Multi-head attention is a crucial component of large language models, allowing them to process and combine information from different parts of the input sequence. In a traditional self-attention mechanism, a single attention head is used to compute the weights for each input element. In contrast, multi-head attention uses multiple attention heads, each computing a different set of weights. This allows the model to capture different types of relationships between input elements, such as syntax, semantics, and pragmatics.\n",
            "\n",
            "The use of multi-head attention in large language models has several benefits. Firstly, it enables the model to capture complex relationships between input elements, which is particularly important for tasks such as machine translation and text summarization. Secondly, it allows the model to focus on different aspects of the input sequence, such as syntax and semantics, which can improve its overall performance. Finally, it enables the model to learn more robust and generalizable representations of the input data.\n",
            "\n",
            "In practice, multi-head attention is typically implemented using a combination of linear transformations and self-attention mechanisms. The input sequence is first transformed using a set of linear transformations, which allows the model to capture different types of relationships between input elements. The transformed input sequence is then passed through a self-attention mechanism, which computes the weights for each input element. The weights are then used to compute the output of the attention mechanism, which is a weighted sum of the input elements.\n",
            "\n",
            "Overall, multi-head attention is a powerful technique for processing and combining information from large input sequences, and it has been widely adopted in the development of large language models.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Retriever] Task output: Multi-head attention is a crucial component of large language models, allowing them to process and combine information from different parts of the input sequence. In a traditional self-attention mechanism, a single attention head is used to compute the weights for each input element. In contrast, multi-head attention uses multiple attention heads, each computing a different set of weights. This allows the model to capture different types of relationships between input elements, such as syntax, semantics, and pragmatics.\n",
            "\n",
            "The use of multi-head attention in large language models has several benefits. Firstly, it enables the model to capture complex relationships between input elements, which is particularly important for tasks such as machine translation and text summarization. Secondly, it allows the model to focus on different aspects of the input sequence, such as syntax and semantics, which can improve its overall performance. Finally, it enables the model to learn more robust and generalizable representations of the input data.\n",
            "\n",
            "In practice, multi-head attention is typically implemented using a combination of linear transformations and self-attention mechanisms. The input sequence is first transformed using a set of linear transformations, which allows the model to capture different types of relationships between input elements. The transformed input sequence is then passed through a self-attention mechanism, which computes the weights for each input element. The weights are then used to compute the output of the attention mechanism, which is a weighted sum of the input elements.\n",
            "\n",
            "Overall, multi-head attention is a powerful technique for processing and combining information from large input sequences, and it has been widely adopted in the development of large language models.\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Answer Grader\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the retriever task for the quetion How does multi-head attention influence large language models? evaluate whether the retrieved content is relevant to the question.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
            "\n",
            "Final Answer: yes\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Answer Grader] Task output: yes\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Hallucination Grader\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the grader task for the quetion How does multi-head attention influence large language models? evaluate whether the answer is grounded in / supported by a set of facts.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
            "Final Answer: yes\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Hallucination Grader] Task output: yes\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Answer Grader\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the hallucination task for the quetion How does multi-head attention influence large language models? evaluate whether the answer is useful to resolve the question.If the answer is 'yes' return a clear and concise answer.If the answer is 'no' then perform a 'websearch' and return the response\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
            "\n",
            "Action: Since the response from the hallucination task is \"yes\", I will return a clear and concise answer.\n",
            "\n",
            "Final Answer: Multi-head attention is a mechanism used in large language models to allow the model to focus on different aspects of the input data simultaneously. It does this by applying different weights to different heads, which are essentially different attention mechanisms. This allows the model to capture complex relationships between different parts of the input data, which can improve its ability to generate coherent and relevant text.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Answer Grader] Task output: Multi-head attention is a mechanism used in large language models to allow the model to focus on different aspects of the input data simultaneously. It does this by applying different weights to different heads, which are essentially different attention mechanisms. This allows the model to capture complex relationships between different parts of the input data, which can improve its ability to generate coherent and relevant text.\n",
            "\n",
            "\u001b[00m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8ra4aAASuHN",
        "outputId": "09583737-8632-4471-9f1d-20504b374f1b"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-head attention is a mechanism used in large language models to allow the model to focus on different aspects of the input data simultaneously. It does this by applying different weights to different heads, which are essentially different attention mechanisms. This allows the model to capture complex relationships between different parts of the input data, which can improve its ability to generate coherent and relevant text.\n"
          ]
        }
      ]
    }
  ]
}